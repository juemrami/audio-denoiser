{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:01<00:00, 93.70it/s] \n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "def WienerFilter(filename, sr=16000, n_fft=512, hop_rate=0.5, vad_db=5, gamma=1.0, G=.6):\n",
    "    x, _ = librosa.load(filename, sr=sr)\n",
    "\n",
    "    hop = int(hop_rate*n_fft)         # hop size in samples\n",
    "\n",
    "    X = librosa.stft(x, n_fft=n_fft, hop_length=hop)\n",
    "    # setting default parameters\n",
    "    vad_db = 5       # VAD vad_dbhold in dB SNRseg\n",
    "    gamma = 1.0     # exp(gamma)\n",
    "    G = .6 #smoothing factor\n",
    "\n",
    "    noise_mean = np.zeros((n_fft//2+1))\n",
    "    for k in range(0, 5):\n",
    "        noise_mean = noise_mean + abs(X[:, k])\n",
    "\n",
    "    # noise estimate from first 5 frames\n",
    "    noise_mu = noise_mean / 5\n",
    "\n",
    "    # initialize various variables\n",
    "    img = 1j\n",
    "    X_out = np.zeros(X.shape, dtype=complex)\n",
    "\n",
    "    # main processing loop\n",
    "    for n in tqdm(range(0, X.shape[1])):\n",
    "        # extract a frame\n",
    "        signal_spec = X[:, n]\n",
    "        # compute the magnitude\n",
    "        signal_magnitude = abs(signal_spec)\n",
    "        # save the noisy phase information\n",
    "        theta = np.angle(signal_spec)\n",
    "        #  compute segmental SNR for VAD\n",
    "        SNRseg = 10 * np.log10(np.linalg.norm(signal_magnitude, 2) ** 2 / np.linalg.norm(noise_mu, 2) ** 2)\n",
    "\n",
    "        # perform the spectral subtraction\n",
    "        clean_signal_magnitude = signal_magnitude ** gamma - noise_mu ** gamma\n",
    "\n",
    "        # halfwave rectification (zero out negative values)\n",
    "        clean_signal_magnitude = np.maximum(clean_signal_magnitude, 0)\n",
    "\n",
    "        # compute a Priori SNR (used)\n",
    "        SNRpri = 10 * np.log10(np.linalg.norm(clean_signal_magnitude, 2) ** 2 / np.linalg.norm(noise_mu, 2) ** 2)\n",
    "\n",
    "        # parameter band dependent oversubtraction factor\n",
    "        mu_max = 20\n",
    "        mu_to_plus, mu_to_min = 1, mu_max\n",
    "        mu_slope = ((mu_to_min - mu_to_plus) * mu_max) / 25\n",
    "        mu_0 = mu_to_plus + 20*mu_slope\n",
    "        def get_alpha(SNR):\n",
    "            if SNR >= 20:\n",
    "                 return mu_to_plus\n",
    "            elif -5.0 <= SNR <= 20.0:\n",
    "                return mu_0 - SNR*mu_slope\n",
    "            else: return mu_to_min\n",
    "        alpha = get_alpha(SNRpri) \n",
    "\n",
    "        # 2 gain function G\n",
    "        # This is essentially the inverse Wiener Filter\n",
    "        G_i = clean_signal_magnitude ** 2 / (clean_signal_magnitude ** 2 + alpha * noise_mu ** 2)\n",
    "        \n",
    "        wf_speech = G_i * signal_magnitude\n",
    "\n",
    "        # --- implement a simple VAD detector --- #\n",
    "        if SNRseg < vad_db:  # Update noise spectrum\n",
    "            noise_temp = G * noise_mu ** gamma + (1 - G) * signal_magnitude ** gamma  # noise power spectrum smoothing\n",
    "            noise_mu = noise_temp ** (1 / gamma)  # New noise amplitude spectrum\n",
    "            clean_signal_magnitude = .2*signal_magnitude  # suppress the signal    \n",
    "        # add phase    \n",
    "        phased_clean_signal = (wf_speech ** (1 / gamma)) * np.exp(img * theta)       \n",
    "        # store the output\n",
    "        X_out[:, n] = phased_clean_signal\n",
    "        signal = librosa.istft(X_out, hop_length=hop, n_fft=n_fft)\n",
    "        outfile = filename.split('.')[0] + '_denoised.wav'\n",
    "        sf.write(outfile, signal, sr)\n",
    "        # return list(signal)\n",
    "WienerFilter('test.wav') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Available: ✅\n",
      "Cuda Available: ✅\n",
      "GPU Available: ✅\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from tensorflow._api.v2.test import is_built_with_cuda, gpu_device_name\n",
    "from tensorflow.python.keras import models\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "import os; os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "\n",
    "is_cuda = is_built_with_cuda()\n",
    "is_gpu = gpu_device_name()\n",
    "model_path = os.path.join(os.getcwd(),'saved','models', 'fcnn_AN.model')\n",
    "error = None\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"Error. Model or Weights not found. Please download the model and weights from the repository and place them in the saved/models folder. Alternatively, you can run the train the model on your own data.\")\n",
    "    exit()\n",
    "model = models.load_model(model_path)\n",
    "print(\"Model Available: \" + ('\\u274C', '\\u2705')[int(model is not None)])\n",
    "print(\"Cuda Available: \" + ('\\u274C', '\\u2705')[int(is_cuda)])\n",
    "print(\"GPU Available: \" + ('\\u274C', '\\u2705')[int(is_cuda)])\n",
    "\n",
    "def DeepDenoise(filename, sr=16000, segment_time=0):\n",
    "    # Segment time is used to split the audio into segments of X seconds for inference\n",
    "    # SR = 16000 is what i used for training as specified in the paper\n",
    "    # So, I'm assuming that's what is appropriate for the inference as well\n",
    "    x, _ = librosa.load(filename, sr=sr)\n",
    "    # print(x.shape)\n",
    "    \n",
    "    # Implement split into segments, its okay if the last segment is less than 1.5 seconds.\n",
    "    if segment_time > 0:\n",
    "        max_samples = int(sr*segment_time)\n",
    "        x_out = []\n",
    "        for i in (range(0, len(x), max_samples)):\n",
    "            x_i = x[i:i+max_samples]\n",
    "            # print (x_i.shape)\n",
    "            x_i = np.expand_dims(np.expand_dims(x_i, axis=0), -1)\n",
    "            x_i_out = model.predict(x_i)\n",
    "            # print (x_i_out.shape)\n",
    "            x_out.append(x_i_out)\n",
    "        x_out = np.concatenate(x_out, axis=1).squeeze()\n",
    "    else:\n",
    "        x_out = model.predict(np.expand_dims(np.expand_dims(x, axis=0), -1)).squeeze()\n",
    "    # print(x_out.shape)\n",
    "    outfile = filename.split('.')[0] + '_ml_denoised.wav'\n",
    "    sf.write(outfile, x_out, sr)\n",
    "\n",
    "DeepDenoise('input.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b41e41bff03a930185ac2cbea60ce521f7c8c2f5401ceb5c30214081f77ab4f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
