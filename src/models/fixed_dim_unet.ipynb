{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 513 96\n",
      "(450, 513, 96) <dtype: 'complex64'> <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(50, 513, 96) <dtype: 'complex64'> <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(450, 513, 96) <dtype: 'complex64'> <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(50, 513, 96) <dtype: 'complex64'> <class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras import backend \n",
    "import os\n",
    "from scipy import stats\n",
    "\n",
    "data_path = os.path.join(os.getcwd(), 'data')\n",
    "X = np.load(os.path.join(data_path, 'X_69_500.npy'), allow_pickle=True)\n",
    "Y = np.load(os.path.join(data_path, 'Y_69_500.npy'), allow_pickle=True)\n",
    "\n",
    "freq_bins = X[0].shape[0]\n",
    "# min_spec_len = min([x.shape[1] for x in X])\n",
    "min_spec_len = 96\n",
    "data_size = X.shape[0]\n",
    "\n",
    "print(data_size, freq_bins, min_spec_len)\n",
    "X = [tf.stack(x[:,:min_spec_len]) for x in X] # noised spectrograms\n",
    "Y = [tf.stack(y[:,:min_spec_len])for i,y in enumerate(Y)] # noised - clean = noise only spectrograms\n",
    "\n",
    "splits = train_test_split(X, Y, test_size=0.1, random_state=69)\n",
    "X_train, X_test, y_train, y_test = [tf.convert_to_tensor(s) for s in splits]\n",
    "for item in [X_train, X_test, y_train, y_test]:\n",
    "    print(item.shape, item.dtype, type(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 513, 96, 16), (None, 512, 96, 16)]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [10], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m up9 \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mConv2D(size_filter_in, \u001b[39m2\u001b[39m, activation \u001b[39m=\u001b[39m activation_layer, padding \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m, kernel_initializer \u001b[39m=\u001b[39m kernel_init)(layers\u001b[39m.\u001b[39mUpSampling2D(size \u001b[39m=\u001b[39m (\u001b[39m2\u001b[39m,\u001b[39m2\u001b[39m))(conv8))\n\u001b[0;32m     64\u001b[0m up9 \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mLeakyReLU()(up9)\n\u001b[1;32m---> 65\u001b[0m merge9 \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39;49mConcatenate(axis \u001b[39m=\u001b[39;49m \u001b[39m3\u001b[39;49m)([conv1,up9])\n\u001b[0;32m     66\u001b[0m conv9 \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mConv2D(size_filter_in, \u001b[39m3\u001b[39m, activation \u001b[39m=\u001b[39m activation_layer, padding \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m, kernel_initializer \u001b[39m=\u001b[39m kernel_init)(merge9)\n\u001b[0;32m     67\u001b[0m conv9 \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mLeakyReLU()(conv9)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\keras\\layers\\merging\\concatenate.py:131\u001b[0m, in \u001b[0;36mConcatenate.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    125\u001b[0m unique_dims \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\n\u001b[0;32m    126\u001b[0m     shape[axis]\n\u001b[0;32m    127\u001b[0m     \u001b[39mfor\u001b[39;00m shape \u001b[39min\u001b[39;00m shape_set\n\u001b[0;32m    128\u001b[0m     \u001b[39mif\u001b[39;00m shape[axis] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    129\u001b[0m )\n\u001b[0;32m    130\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(unique_dims) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 131\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err_msg)\n",
      "\u001b[1;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 513, 96, 16), (None, 512, 96, 16)]"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from tensorflow import keras\n",
    "#Unet network\n",
    "data_shape = (freq_bins, min_spec_len)\n",
    "\n",
    "\n",
    "size_filter_in = 16\n",
    "kernel_init = 'he_normal'\n",
    "activation_layer = None \n",
    "model_input = layers.Input(shape=(freq_bins, min_spec_len), dtype=np.complex64)\n",
    "inputs = tf.expand_dims(model_input, axis=-1)\n",
    "inputs = tf.abs(inputs)\n",
    "conv1 = layers.Conv2D(size_filter_in, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(inputs)\n",
    "conv1 = layers.LeakyReLU()(conv1)\n",
    "conv1 = layers.Conv2D(size_filter_in, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv1)\n",
    "conv1 = layers.LeakyReLU()(conv1)\n",
    "pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = layers.Conv2D(size_filter_in*2, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(pool1)\n",
    "conv2 = layers.LeakyReLU()(conv2)\n",
    "conv2 = layers.Conv2D(size_filter_in*2, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv2)\n",
    "conv2 = layers.LeakyReLU()(conv2)\n",
    "pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = layers.Conv2D(size_filter_in*4, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(pool2)\n",
    "conv3 = layers.LeakyReLU()(conv3)\n",
    "conv3 = layers.Conv2D(size_filter_in*4, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv3)\n",
    "conv3 = layers.LeakyReLU()(conv3)\n",
    "pool3 = layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "conv4 = layers.Conv2D(size_filter_in*8, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(pool3)\n",
    "conv4 = layers.LeakyReLU()(conv4)\n",
    "conv4 = layers.Conv2D(size_filter_in*8, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv4)\n",
    "conv4 = layers.LeakyReLU()(conv4)\n",
    "drop4 = layers.Dropout(0.5)(conv4)\n",
    "pool4 = layers.MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "conv5 = layers.Conv2D(size_filter_in*16, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(pool4)\n",
    "conv5 = layers.LeakyReLU()(conv5)\n",
    "conv5 = layers.Conv2D(size_filter_in*16, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv5)\n",
    "conv5 = layers.LeakyReLU()(conv5)\n",
    "drop5 = layers.Dropout(0.5)(conv5)\n",
    "\n",
    "up6 = layers.Conv2D(size_filter_in*8, 2, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(layers.UpSampling2D(size = (2,2))(drop5))\n",
    "up6 = layers.LeakyReLU()(up6)\n",
    "merge6 = layers.Concatenate(axis = 3)([drop4,up6])\n",
    "conv6 = layers.Conv2D(size_filter_in*8, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(merge6)\n",
    "conv6 = layers.LeakyReLU()(conv6)\n",
    "conv6 = layers.Conv2D(size_filter_in*8, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv6)\n",
    "conv6 = layers.LeakyReLU()(conv6)\n",
    "up7 = layers.Conv2D(size_filter_in*4, 2, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(layers.UpSampling2D(size = (2,2),)(conv6))\n",
    "up7 = layers.LeakyReLU()(up7)\n",
    "merge7 = layers.Concatenate(axis = 3)([conv3,up7])\n",
    "conv7 = layers.Conv2D(size_filter_in*4, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(merge7)\n",
    "conv7 = layers.LeakyReLU()(conv7)\n",
    "conv7 = layers.Conv2D(size_filter_in*4, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv7)\n",
    "conv7 = layers.LeakyReLU()(conv7)\n",
    "up8 = layers.Conv2D(size_filter_in*2, 2, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(layers.UpSampling2D(size = (2,2))(conv7))\n",
    "up8 = layers.LeakyReLU()(up8)\n",
    "merge8 = layers.Concatenate(axis = 3)([conv2,up8])\n",
    "conv8 = layers.Conv2D(size_filter_in*2, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(merge8)\n",
    "conv8 = layers.LeakyReLU()(conv8)\n",
    "conv8 = layers.Conv2D(size_filter_in*2, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv8)\n",
    "conv8 = layers.LeakyReLU()(conv8)\n",
    "\n",
    "up9 = layers.Conv2D(size_filter_in, 2, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(layers.UpSampling2D(size = (2,2))(conv8))\n",
    "up9 = layers.LeakyReLU()(up9)\n",
    "merge9 = layers.Concatenate(axis = 3)([conv1,up9])\n",
    "conv9 = layers.Conv2D(size_filter_in, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(merge9)\n",
    "conv9 = layers.LeakyReLU()(conv9)\n",
    "conv9 = layers.Conv2D(size_filter_in, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv9)\n",
    "conv9 = layers.LeakyReLU()(conv9)\n",
    "conv9 = layers.Conv2D(2, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv9)\n",
    "conv9 = layers.LeakyReLU()(conv9)\n",
    "conv10 = layers.Conv2D(1, 1, activation = 'tanh')(conv9)\n",
    "\n",
    "model = keras.Model(model_input,conv10)\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.Huber(), metrics = ['mae'])\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "# if(pretrained_weights):\n",
    "#     model.load_weights(pretrained_weights)\n",
    "\n",
    "# return model\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
